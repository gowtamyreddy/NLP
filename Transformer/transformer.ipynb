{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhRwUSs6KIOrv5pJQVB8fz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowtamyreddy/NLP/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding Transformer in decoder format.\n",
        " *This Decoder is excluding Masking from a regular Decoder.*\n",
        "\n",
        " *This model can see the fututre words during training.*\n",
        "\n",
        " *This Model has a single Transformer block.*"
      ],
      "metadata": {
        "id": "Z28zWuiSBfMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ty9v3cBtBciR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "import numpy as np\n",
        "\n",
        "#load the harry potter book as the dataset ->  url - https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n",
        "def load_data(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "  return text\n",
        "\n",
        "file_path = \"/content/sample_data/01 Harry Potter and the Sorcerers Stone.txt\"\n",
        "text = load_data(file_path).lower()\n",
        "\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# Convert text to sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "seq_length = 50\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
        "\n",
        "#print(input_sequences[0])\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "zN3l2jlQBS-p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer model\n"
      ],
      "metadata": {
        "id": "4pQwHp3qBX9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # example - 8\n",
        "\n",
        "        self.embed_dim = embed_dim # example - 512\n",
        "        # embed_dim = dimension of Q, K, and V before splitting into multiple heads\n",
        "        self.projection_dim = embed_dim // num_heads # Size of Each Attention Head's Subspace\n",
        "        self.query_dense = Dense(embed_dim) # Q Determines \"what to focus on\"\n",
        "        self.key_dense = Dense(embed_dim) # K Acts as \"labels\" to be matched with queries\n",
        "        self.value_dense = Dense(embed_dim) # V Holds the actual information\n",
        "\n",
        "        self.combine_heads = Dense(embed_dim)\n",
        "\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        scores = tf.matmul(query, key, transpose_b=True)\n",
        "        scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # converting integer to a float32 tensor\n",
        "\n",
        "        attention_probs = tf.nn.softmax(scores, axis=-1) # how much attention each token should give to other tokens\n",
        "        return tf.matmul(attention_probs, value), attention_probs\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value = inputs\n",
        "        batch_size = tf.shape(query)[0] # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        query = self.split_heads(self.query_dense(query), batch_size)\n",
        "        key = self.split_heads(self.key_dense(key), batch_size)\n",
        "        value = self.split_heads(self.value_dense(value), batch_size)\n",
        "\n",
        "        attention, _ = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "        return self.combine_heads(concat_attention)\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att([inputs, inputs, inputs])\n",
        "\n",
        "        # Dropout randomly deactivates some neurons during training to reduce overfitting\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output) # Residual Connection\n",
        "\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        maxlen = tf.shape(x)[-1] # sets maxlen to the length of the input sequence\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1) # Generate [0, 1, 2, ..., maxlen-1]\n",
        "        positions = self.pos_emb(positions) # Each position index is mapped to a trainable embedding of shape (maxlen, embed_dim)\n",
        "        x = self.token_emb(x) # Each token ID in x is mapped to an embedding of shape (batch_size, maxlen, embed_dim)\n",
        "        return x + positions\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "KJLRsbyjB1V3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "embed_dim = 128  # Embedding size\n",
        "num_heads = 4    # Number of attention heads\n",
        "ff_dim = 512     # Feed-forward layer size\n",
        "maxlen = seq_length # here it is 50 defined above\n",
        "\n",
        "\n",
        "# Build the model\n",
        "inputs = tf.keras.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "print(x.shape)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x, training=True)\n",
        "print(x.shape)\n",
        "x = x[:, -1, :]\n",
        "print(x.shape)\n",
        "x = Dense(total_words, activation=\"softmax\")(x)\n",
        "print(x.shape)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "cLaM4cMbB2A7",
        "outputId": "93d1a0ac-3b67-4f5d-c76f-e8056248e15a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 50, 128)\n",
            "(None, 50, 128)\n",
            "(None, 128)\n",
            "(None, 6663)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m859,264\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m198,272\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6663\u001b[0m)                │         \u001b[38;5;34m859,527\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">859,264</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6663</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">859,527</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, batch_size=32, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z15Bd8lYB6od",
        "outputId": "22a16eee-9622-4f19-db59-eff8f220839d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 70ms/step - accuracy: 0.0840 - loss: 6.5001\n",
            "Epoch 2/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 70ms/step - accuracy: 0.1612 - loss: 5.0723\n",
            "Epoch 3/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 69ms/step - accuracy: 0.2114 - loss: 4.2803\n",
            "Epoch 4/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 70ms/step - accuracy: 0.2569 - loss: 3.6334\n",
            "Epoch 5/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 70ms/step - accuracy: 0.3186 - loss: 3.1030\n",
            "Epoch 6/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 71ms/step - accuracy: 0.3991 - loss: 2.6039\n",
            "Epoch 7/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 72ms/step - accuracy: 0.4792 - loss: 2.1852\n",
            "Epoch 8/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 71ms/step - accuracy: 0.5494 - loss: 1.8167\n",
            "Epoch 9/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 71ms/step - accuracy: 0.6124 - loss: 1.5229\n",
            "Epoch 10/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 71ms/step - accuracy: 0.6648 - loss: 1.2834\n",
            "Epoch 11/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 71ms/step - accuracy: 0.7075 - loss: 1.0939\n",
            "Epoch 12/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 72ms/step - accuracy: 0.7476 - loss: 0.9281\n",
            "Epoch 13/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 70ms/step - accuracy: 0.7787 - loss: 0.7935\n",
            "Epoch 14/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 71ms/step - accuracy: 0.8050 - loss: 0.6881\n",
            "Epoch 15/15\n",
            "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 70ms/step - accuracy: 0.8303 - loss: 0.5896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"harry at Hogwarts\"\n",
        "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
        "print(len(generated_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlDUMb17B_yn",
        "outputId": "1d2cfcc2-6458-4ec7-bab2-0010eb7e8aef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2qhK602NKoi",
        "outputId": "545b70c2-7615-4316-e66b-1d73aaa61481"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harry at Hogwarts and he heard of course books the first few days mr dursley however was going to do he put it back in his mind as he went on to bed and stared at him if he wouldn’t be able to see if he was doing very that he was wearing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"harry and Snape\"\n",
        "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUYWy5fTNO65",
        "outputId": "6915a02b-f204-4347-84f0-71a610c9a71d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmp3OHTMTgd_",
        "outputId": "8d4b2f5c-8601-4d33-83c6-f4162e675e80"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harry and Snape went on the table piled so teach him magic tricks and yelled uncle vernon but he had finally gone he hadn’t brought the umbrella swishing down through his newspaper at hagrid and had pulled the wild cart ride later in this was empty there was he looking stunned and stroked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"harry in the team of gryffindor\"\n",
        "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNA3D5oKTm1Y",
        "outputId": "8731ce2e-a2f1-40d3-c8e3-2174f55356fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harry in the team of gryffindor meant to turn up at first years it’s really dangerous with the sorcerer’s stone and neville was the very quiet until he was wearing bits of wood and reach them he had a few seconds later he had complete twelve the thought snape would never have been sick on as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"Harry,Ron\"\n",
        "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzxSVjteTxvC",
        "outputId": "68e6d37e-f546-4000-cb6c-b4f48a2c3cce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry,Ron and hermione at all over the bundle of blankets and then came floating in midair over the floor it was standing on the floor and tiptoed toward the trophy room malfoy and crabbe weren’t gringotts the candles in the direction that ron wasn’t field where hermione was going to tell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6Qk2KqLUNFo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
